{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81cd49d3-dd79-44a9-a087-b1ca95d0acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c38e8c30-8c9b-4a99-a718-863e05073ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading GoEmotions dataset (optimized sample)...\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# Load GoEmotions Dataset (Reduced Sample)\n",
    "# =====================================\n",
    "print(\"ðŸ“¥ Loading GoEmotions dataset (optimized sample)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "dataset = load_dataset(\"go_emotions\", \"simplified\")  # 6 emotions + neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "877844c1-4604-4ba6-b39b-b82514a89539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset sizes:\n",
      "Train: 43410, Val: 5426, Test: 5427\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL OPTIMIZATION: Use much smaller samples\n",
    "print(\"Original dataset sizes:\")\n",
    "print(f\"Train: {len(dataset['train'])}, Val: {len(dataset['validation'])}, Test: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12edf05d-8cea-4c2c-b8c5-d9e4bbde3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dataset size dramatically for speed\n",
    "TRAIN_SAMPLE = 3000  # Instead of 43k+\n",
    "VAL_SAMPLE = 800     # Instead of 5k+\n",
    "TEST_SAMPLE = 1000   # Instead of 5k+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30b5b6ca-33b3-4481-aecc-f49574b9fab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dataset sizes:\n",
      "Train: 3000, Val: 800, Test: 1000\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(dataset[\"train\"]).sample(n=TRAIN_SAMPLE, random_state=42)\n",
    "val_df = pd.DataFrame(dataset[\"validation\"]).sample(n=VAL_SAMPLE, random_state=42)\n",
    "test_df = pd.DataFrame(dataset[\"test\"]).sample(n=TEST_SAMPLE, random_state=42)\n",
    "\n",
    "print(f\"Reduced dataset sizes:\")\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c26357e-a4d6-4e65-baf0-05fcc7943d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process labels\n",
    "train_df[\"label\"] = train_df[\"labels\"].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "val_df[\"label\"] = val_df[\"labels\"].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "test_df[\"label\"] = test_df[\"labels\"].apply(lambda x: x[0] if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56662353-f50e-4062-b9f4-42b9d4d23c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df[\"label\"] = label_encoder.fit_transform(train_df[\"label\"])\n",
    "val_df[\"label\"] = label_encoder.transform(val_df[\"label\"])\n",
    "test_df[\"label\"] = label_encoder.transform(test_df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1a8496e-e9a5-4d2e-8eab-a7554b0d99fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared. Classes: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(26), np.int64(27)]\n",
      "Data loading time: 222.63 seconds\n"
     ]
    }
   ],
   "source": [
    "# Keep only text and label columns\n",
    "train_df = train_df[[\"text\", \"label\"]].reset_index(drop=True)\n",
    "val_df = val_df[[\"text\", \"label\"]].reset_index(drop=True)\n",
    "test_df = test_df[[\"text\", \"label\"]].reset_index(drop=True)\n",
    "\n",
    "print(\"Dataset prepared. Classes:\", list(label_encoder.classes_))\n",
    "print(f\"Data loading time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "725314b7-62e4-4776-b0fe-871275140fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing texts...\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# Tokenization (Optimized)\n",
    "# =====================================\n",
    "print(\"Tokenizing texts...\")\n",
    "tokenize_start = time.time()\n",
    "\n",
    "save_path = \"./fast_emotion_model\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# OPTIMIZATION: Shorter max_length for speed\n",
    "MAX_LENGTH = 64  # Instead of 128 or 512\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=MAX_LENGTH\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0abc7c96-ea2f-463c-b235-9fe34fbfdd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af84c5f34ace490fa0ac65ae53b49881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1863065070524da2abb7d808f4e2674f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13578699a7cd4fbeaed51b00af08041b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert to HuggingFace datasets and tokenize\n",
    "train_dataset = Dataset.from_pandas(train_df).map(tokenize_batch, batched=True)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(tokenize_batch, batched=True)\n",
    "test_dataset = Dataset.from_pandas(test_df).map(tokenize_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa5247b5-75d8-4ff2-a07d-4b3aaa11705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename label column for trainer\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "val_dataset = val_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b302c9a8-c190-4e24-8b6e-1e045a26d7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Time: 47.48 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set format for PyTorch\n",
    "columns_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_dataset.set_format(\"torch\", columns=columns_to_keep)\n",
    "val_dataset.set_format(\"torch\", columns=columns_to_keep)\n",
    "test_dataset.set_format(\"torch\", columns=columns_to_keep)\n",
    "\n",
    "print(f\"Tokenization complete. Time: {time.time() - tokenize_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59ff003a-eb94-40de-83f0-ef4d0d72b517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model...\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# Model Setup and Optimization\n",
    "# =====================================\n",
    "print(\"Setting up model...\")\n",
    "model_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a7df40c-627d-44bf-bb32-16ce1a4615a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training new model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„ï¸ Freezing early layers for speed...\n"
     ]
    }
   ],
   "source": [
    "# Check if model already exists\n",
    "if os.path.exists(save_path):\n",
    "    print(\"ðŸ“‚ Loading existing model from disk...\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(save_path)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(save_path)\n",
    "    print(\"Model loaded from disk!\")\n",
    "else:\n",
    "    print(\"Training new model...\")\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", \n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    \n",
    "    # CRITICAL OPTIMIZATION: Freeze early layers for faster training\n",
    "    print(\"Freezing early layers for speed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf8e15ea-9d2c-42f4-a535-5ef868ced156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 66,975,004\n",
      "Trainable parameters: 22,270,492 (33.3%)\n",
      "Total parameters: 66,975,004\n",
      "Trainable parameters: 21,877,276 (32.7%)\n",
      "Total parameters: 66,975,004\n",
      "Trainable parameters: 21,876,508 (32.7%)\n",
      "Total parameters: 66,975,004\n",
      "Trainable parameters: 21,875,740 (32.7%)\n"
     ]
    }
   ],
   "source": [
    "# Freeze embeddings\n",
    "for param in model.distilbert.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "    # Freeze first 3 transformer layers (out of 6)\n",
    "    for i in range(3):\n",
    "        for param in model.distilbert.transformer.layer[i].parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c35a3508-5bd7-43cb-8a58-054d1eb0ede7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 21:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.831800</td>\n",
       "      <td>2.418839</td>\n",
       "      <td>0.373750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.206100</td>\n",
       "      <td>2.256636</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 21.2 minutes!\n"
     ]
    }
   ],
   "source": [
    " # Metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "\n",
    "# OPTIMIZED TRAINING ARGUMENTS for speed\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fast_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",  # Don't save intermediate checkpoints\n",
    "    learning_rate=5e-5,  # Higher learning rate for faster convergence\n",
    "    per_device_train_batch_size=32,  # Larger batch size for speed\n",
    "    per_device_eval_batch_size=64,   # Even larger for evaluation\n",
    "    num_train_epochs=2,  # Fewer epochs\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=False,\n",
    "    dataloader_num_workers=0,  # Important: 0 for CPU\n",
    "    remove_unused_columns=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=None,  # Disable wandb/tensorboard\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "train_start = time.time()\n",
    "trainer.train()\n",
    "train_time = time.time() - train_start\n",
    "print(f\"Training completed in {train_time/60:.1f} minutes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f9692d-589d-4ec5-9cad-8517f87dbb84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce55b8-1591-42f9-9c2d-92c9cce2c882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
